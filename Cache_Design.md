# Cache Design

### 基本设计概览

#### 地址编码

高速缓存的设计为8KB的二路组相联，块大小设为32个字节，即8个字，每个字为一个bank，首先给出以下的32位物理地址编码规则。

###### Tag位（物理地址高位）：20位		Index位（组地址）：7位		Offset位（块内偏移）：5位

对于额外的标志位，有以下的设定。

###### Valid位（数据是否有效）：1位		Dirty位（脏标志）：一位

每一组有两路，共用一个LRU位。

###### LRU位（最近最少替换标志）：1位

脏标志是只有在弹出的时候才会使用，与Valid和Tag的使用方式区别较大，所以将其分开，最终，块的数据编码如下：

| Tag+Valid | Bank0 | Bank1 | Bank2 | Bank3 | Bank4 | Bank5 | Bank6 | Bank7 | Dirty |
| :-------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|   20+1    |  32   |  32   |  32   |  32   |  32   |  32   |  32   |  32   |   1   |


#### 主存写策略

当Cache写入主存中更新其数据的时候，采用的方式是写回方式，需要有脏数据的标记。当执行主存写指令的时候，若其数据存于Cache之中，则将Cache中的数据进行更改，并且同时将其所在块标记为“**脏**”。等到进行Cache数据替换脏块的时候，将脏块写入进内存中。

但是当Cache中没有指定的数据，那么首先从主存中读取指定的数据，再将其标记为脏。该策略被称为Write Allocate。

#### Cache数据替换策略

当Cache未命中的时候，或者说需要从主存中读数据写入Cache的时候，采用的策略是伪LRU（伪近期最少使用法），对于2路组相联，对每一组（set）都设置了一位的LRU标志，为0表示way0最近没有被使用，为1同理。LRU位决定了替换该组中的块时被替换的那一个。注意，被替换的数据若是dirty的话，就必须写入内存。

#### 整体的处理流程

为了增加器件复用的程度，设计中的读写整体处理流程都是追求一致的，具体如下，来自《超标量处理器设计》。

![image-20200629201539520](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200629201539520.png)

### 流水线设计

Cache的将从状态机版本转化为更为先进的三级流水线设计，三级流水线的设计相较于传统的两级设计来说，最大贡献就是避免了过高的组合延迟。在第一个状态，也就是look up中，FIFO的探查，以及TLB（MMU）的虚实地址的转换非常费时。

在三级中，多出来的第二级使得我们能够做合理的拆分，将原来的数据RAM和TAG RAM同时访问变为串行访问，先访问TAG，然后判断完TAG之后才访存对应的RAM，这样可以省去比较器的延时，也可以给TLB和FIFO以喘息时间。

在第三级中，我们将数据进行输出，如果命中失败，那么这个时候就需要在第三级中对总线发出读请求，同时发出cache以及cpu流水线的暂停请求，直到访存完毕。值得注意的是，第三级同样是写命令的数据准备好进入FIFO的时候，那么这个时候如果FIFO满了，也要暂停流水。



### 硬件架构设计



#### ICache接口设置

输入：

###### 全局信号：时钟clk，复位rst

###### CPU指令读写命令：读使能cpu_req_i，虚拟地址virtual_addr_i

###### CPU暂停控制：stall_i

输出：

###### CPU指令读命令结果：是否命中cache hit_o，当前数据是否有效 inst_valid_o，当前输出的数据cpu_inst_o

###### CPU控制信号：取数据暂停信号stall_o，取值结果只支持单发single_shot

###### 总线slave信号：读数据有效mem_rvalid_i，读地址可以接收mem_arready_i，主存送入的块数据mem_rdata_i

###### 总线master信号：读使能mem_ren_o，读数据可以接收mem_rready_o，读地址有效mem_arvalid_o，读地址mem_araddr_o

注意：

###### hit_o：在req_i给出的下一个周期给出，高电平表示命中，低电平表示不命中

#### DCache接口设置

输入：

###### 全局信号：时钟clk，复位rst

###### CPU指令读写命令：读使能cpu_rreq_i，写使能cpu_wreq_i，虚拟地址virtual_addr_i，写数据cpu_wdata_i

输出：

###### CPU指令读命令结果：是否命中cache hit_o，当前数据是否有效 inst_valid_o，当前输出的数据inst_o

###### 总线slave信号：读数据有效mem_rvalid_i，读地址可以接收mem_arready_i，主存送入的块数据mem_rdata_i

###### 总线master信号：读使能mem_ren_o，读数据可以接收mem_rready_o，读地址有效mem_arvalid_o，读地址mem_araddr_o

注意：

###### hit_o：在req_i给出的下一个周期，其数据有效

#### Cache_Ram的构造

为了存放相应的数据，对于每一路构造以下结构的Block Ram：

###### Tag+Valid：21位*128块\*2路（实际为了对齐8位采用24位）

###### Bank0~7：32位*128块\*2路

对于脏位和LRU则采用寄存器直接构建：

###### Dirty：1位*128块\*2路

###### LRU：1位*128块

关于地址，由Index指定块的深度（也就是组序号），由Offset指定块内偏移，下面具体明确与地址的对应关系：

###### Tag+Valid,Dirty的ram地址:Index

###### Tag+Valid,Dirty的ram选择使能:永远为真（两路全部取出）

###### Bank的ram地址：Index

###### Bank的具体使能：用B<sub>i</sub>W<sub>j</sub> 表示第i个bank，第j路，则有Offset[4:2]指明第i个bank，两路使能相同。

###### Dirty的ram地址：Index。

###### //这里应该要有一个3D的说明图，来表示RAM的构造关系

对于TagV ram，其中写入的数据永远是物理地址，只要将写入数据段与物理地址相接就可以了。

在非流水Cache中，对于ram来说，不会发生同时写和读的情况，但是在流水Cache中则会发生。ram必须直接组合逻辑连接于输入的addr，以保证一周期反应，那么在非流水Cache中就需要进行输入addr的选择，以保证能写能读。在流水Cache中，则需要使用多端口的ram同时进行读写，同时注意在同时进行读写的时候，如果两端口地址相同，输出的读数据将直接使用写入的数据（这部分需要自行实现），这部分是IP核自身的collision问题，需要进行规避。（顺带提醒：simple-dual ram完全等价于只开放A的write和B的read的true-dual ram）

#### 主存读命令：命中Cache

- 首先，将用TLB换算出来的物理地址拿出。
- 经过地址寻址，交付给cache_ram取出对应的数据。
- 进行对比确定是否命中，命中则返回。

#### 主存读命令：未命中Cache

- 读出ram的数据如果是脏块的话不要丢弃，**要送给WriteBuffer进行写入（这里还没有实现）**。
- 相应地址使能等交付给总线，之后开始等待。
- 数据返回后送出，同时将数据送给ram进行写回。

#### 主存写命令：命中Cache

- 首先，将用TLB换算出来的物理地址拿出。
- 经过地址寻址，交付给cache_ram取出对应的数据。
- 进行对比之后，将拿出的数据进行重写。
- 将数据进行写回。

#### 主存写命令：未命中Cache

- 首先，将用TLB换算出来的物理地址拿出。并且同时用虚拟地址地位赋予ram取数据，同时将物理地址给FIFO。
- 如果发现ram和FIFO都未命中，转

#### WriteBuffer: FIFO

该部件实质上是用来与总线进行数据交互的，作为写缓存，避免了流水和cache的堵塞，提高性能，但是要注意与cache的联合会出现以下的功能问题。

等待写回的数据在FIFO之中。若在写回之前，对于队列中的数据又出现了读/写操作，则需要将相应的数据抽离出来（这里头的才是最新数据，否则会出现数据错误），这不但可以加速读写的进程，还可以减少总线和FIFO的压力。

值得提出的是由于是以块为数据单位，那么地址位的比较就要做一定的额外处理。为了保证尽可能的低耦合性，对于任意的地址输入输出，都将Offset置0。

WriteBuffer存满后再接受块的时候是**不会拒绝**的，因此会出现溢出，这部分需要由DCache进行管控，在遇到这种情况的时候进行暂停。



#### CacheAXI_Interface

由于Cache送往总线虽采用了握手，但是考虑到功能实现以及其模块化处理，Cache最终以块为单位数据与总线进行沟通，因此我们还需要设置一个中间的Interface，也就是该模块的目的。该模块将使得Cache能够忽略与总线交互时所发生的猝发以及具体的交互信号，将重心放在核心逻辑上，避免无关代码扰乱视听。同时，总线的数据访问不接受同时读，因此我们需要处理ICache和DCache的读冲突（写不会冲突）。我们的处理策略是，访存优先级大于取值。

该模块将块拆成多个字一个一个送入总线之中（写），也可以将总线传回来一个一个字进行块组装，再送回总线中（读）。同时考虑到读冲突，我们要设立优先级。

### 第一代Cache

ICache的输入输出接口均为半成品，能够支持完成一定的功能，但是不能够进行完整的接口配置。其内部功能已经初步仿真正确，并且可综合和实现。

##### 首先，采用状态机而非流水对对应的模块进行开发，读数据状态分配如下：

- Look Up: 将地址传给TLB进行虚实地址转换，并将该地址给予相应的RAM。下一阶段进入Scan Cache
- Scan Cache: 取出对应的cache数据，判断是否命中，将命中结果传出，若命中将数据也传出。命中后跳转到状态Look Up，没有则跳转到Read Fail。
- Hit Fail: 等待至数据返回，（传递数据和地址给ram，下个周期才会写回），然后返回状态Look Up。
- Write Back:拿到总线的数据之后直接将CPU需要的对应的数据送出，同时将对应的数据写回至cache中，下个周期返回状态Look Up。

##### DCache状态机：

- Look Up: 将地址传给TLB进行虚实地址转换，并将该地址给予RAM。下一阶段进入Fetch Data
- Fetch Data: 取出对应的cache数据，判断是否命中，若**读命中**,进行输出，之后跳转到Look Up，若**写命中**，设置好数据送入ram进行写入，之后进入Look Up。若**读不命中**，则与总线交互等待数据，之后进行关键字输出，跳转至Look Up。若**写不命中**，同读不命中，但是写入数据必须考虑到cpu要求的写数据。若有脏块，进入Write Data。
- Write FIFO: Fetch Data所获取的数据（写则需要加上cpu写数据的修改）写入FIFO。
- 对于不同阶段，都有固有的组合逻辑器件进行操作，状态机只相当于“流水线”中间分割的寄存器，但是为了功能的实现，目前的状态机不能够直接切分成流水。

##### 写ram的使能需要在以下情况打开：

- 写命中：将命中位置高位
- 读写不命中：在返回数据的时候将被替换位至高

##### 将写入ram放在Fetch Data的最后一个周期，其中写数据的来源有如下情况：

- 读未命中：总线数据
- 写未命中：总线数据与脏块拼接
- 写命中：分0路、1路、FIFO命中三种，分别拼接数据
- 对于LRU的变换法，其中的数据在以下情况需要变动：
- 非FIFO命中：LRU指向没被命中的块
- 未命中：由于主存取块覆盖在了原来的LRU指向位置，于是将对应位置的LRU取反即可，表示下次覆盖块为另一块。

##### 对于脏标志的设置，在以下情况需要变动：

- 读未命中：对应替换块脏位设0
- 写未命中：对应替换块脏位设为1
- 写命中但是不是FIFO命中：对应替换块脏位设为1（该情况要注意）

##### 对于主存写入，在以下情况会出现：

- 读未命中：若弹出块（LRU决定）为脏块，则送入FIFO（使能也要有效）

- 写未命中：若弹出块（LRU决定）为脏块，则送入FIFO（使能也要有效）

- 写命中FIFO（特别注意）：若写入的块在FIFO中，那么我们就需要将写入块送入FIFO(如果过了一个周期，原来取得的块写入到主存中去了，也没有关系，FIFO会将其加入队尾等待写入)

##### 写入FIFO的数据来源：

- 写命中FIFO：此时将FIFO读出命中的块与写数据合并送入FIFO再次进行覆盖写入。
- 脏块弹出：将LRU_pick指向的数据送入FIFO写入，但是要注意的是AXI取出的输出写入了ram之后，脏块就丢失了，因此需要在AXI取数数据（bus_read_success）的时候将ram中的块拿出来，等待时钟沿的来到。

##### WriteBuffer 冲突：

特别地，对于WriteBuffer，我们注意到写替换命中会出现冲突，当写命中的数据正在被写入总线，就可能会使得我们想写入的数据没有进入总线。我们采用的处理策略是：读写命中分开判断，但是判定逻辑很相似，即读命中且头指针所在位没有命中可以表示写命中。

Read/Write Double Hit Collision：读写双命中意味着FIFO中有两个相同地址块，很明显其中一个正在被写入，这种情况下系统相会失去判断能力甚至给予错误的数据。这代表WriteBuffer存在着根本性的功能错误，是失败的产品。



### 第二代Cache

首先需要进行说明的是，Cache第二代是从半成品迈向成品的全新的一代，它拥有主体部分、写缓存FIFO、CacheAXI接口，不仅如此，在功能上它成为了完整的Cache产品，能够满足具体的使用，我们可以预见，alpha第二代将会迈向beta第一代，而这一代不仅将在功能上表现全面，还会在优化上稍有建树，包括有关键字处理技术、设计精良的三状态式Cache，仅仅400行代码的DCache主体等。在提出美好的愿景之后，我们仍然面临着许多需要修改的内容，下面对现在已经实现的结构已经将来需要实现的结构做一定的说明。

##### 关键字预取（待实现）：

在总线的块读取过程中，一旦发现这是我们需要的字，立刻取出送给Cache。但是这里需要的改动需要在cpu中进行对应结构的改变，因此，我们首先不考虑这样的功能，我们会在功能测试之后进行相关的开发。

##### 信号的优化（已实现）：

显而易见，为了考虑完整的总线接口请求，在第一代的Cache之中，我们使用的是AXI总线的naive共同协议，但是在Interface的加持之下，我们似乎可以对其进行sram化，更为精简，只保留最基本的使能和AXI操作完成标志。

##### Interface（已实现）：

写和读不冲突，但是inst和data的读会冲突，因此，我们采用类似于状态机的写法进行状态的跳转，在遇见inst 和data同时做请求的时候，我们将会给予data以优先权。我们知道，写不会出现busy的状态（FIFO会处理好这一切），但是读会出现。对于读冲突，我们之后会使用关键字技术，则不会等到完全读主存结束才放行Cache，因此我们之后需要单独判断暂停位告诉Cache我们还没有完成，但是现在，我们暂时不需要。

##### 暂停（已实现）：

对于CPU来说，Cache读不命中时和写访存堵塞（同时处理写指令）的时候，需要进行暂停，等待Cache结束相关操作。对于Cache来说，需要解决写访存堵塞（同时处理写指令）的状态调整。

CPU会在**执行**阶段给DCache发送访存相关信息，然后Cache会在**访存**阶段送回数据（顺利的话），但是现在单发射的逻辑是访存阶段给予访存相关信号，这就意味这同时需要进行单发射CPU的逻辑改正，否则会拖累性能。下面我们还是考虑**执行**阶段给予访存信号的情况。

DCache暂停时，即两周期解决不了读写指令，有两种情况，一种是cpu读指令读不命中需要等待，一种是写冲突。特别对于写冲突需要进行说明，由于写不需要返回给CPU数据，可以让CPU先行，写命令和读不命中写脏块自己执行，但是其中会遇到问题：写脏块中途cpu又提出了访存的要求，FIFO满了的时候当前又出现了没有hit的脏块写入（Write Data状态）

综上，暂停可以用状态表示：

- 非Look Up情况，不接受任何CPU的req，发现就堵塞

- Fetch Data：当该状态没有hit的时候，是命中失败了，对于读肯定是要堵塞，但是对于写就不必要了。

- Write Data：这个状态一定是未命中（写脏块）的状态，但是由于是写入FIFO，出现FIFO堵塞就要设置暂停。

##### WriteBuffer重构（已实现）：

在之前的WriteBuffer之中，我们采用的是重复写入进行写命中正在写入块(之后简称为**写冲突**）进行冲突处理，但是我们会遇见非常棘手的问题，因此我们将重新审视我们的代码，进行重构。

为了根本上解决已有问题，我们将不再进行重复写入，而是覆盖，保证FIFO中不存在同地址的新老数据。但是由于写操作猝发不能够中断，因此遇到**写冲突**的时候我们需要等待第一次写完之后再重新写一遍。因此我们需要重新加入一个标志位来解决这种问题。

##### 写冲突（Write Collision）（已解决）：

当写操作命中队头的时候，将设置重写标志（rewrite），这个标志将会使得FIFO的队头在写完之后不移动，使得再重写一遍。要注意设置相关信号：valid和head在rewrite为高位的时候保持自身的valid。

当队头刚好写完的时候（bvalid为真），传入了一个write collision，这个时候从理论设计上不进行复写。实际上由于之前write_hit在队头处为真下一周期会出现：rewrite依然为低电平（正确），头指针后移（正确），头指针处设为invalid（正确），尾指针不后移（错误），队头保持valid（错误！）

所以采取修正措施，让其复写。使得下一周期会出现头指针不后移，并保持valid。

### 第三代Cache

第三代Cache主要关注的问题是“流水”和更为先进的优化技术，我们将着手进行让Cache充分发挥其性能。具体的任务有：命名规范化、ICache流水化、ICache 指令预取（FIFO）、Interface关键字技术。

#### Interface关键字技术

Interface在与主存进行操作的时候，如果获得了对应字的数据则先行输出，不等整块读出再从块中进行选择。但是在具体来说，实现的时候会出现一些问题。注意该技术对于DCache更有用（访问频度不高），但是对于ICache（持续访问），即使先行送出，但是块打包的过程并未结束，就算提前送出，下一条指令也会收到影响无法流动。

ICache：ICache进行主存读取的时候，注意到Interface使用了一个块的寄存器一个字一个字的接受总线返回的数据，那么我们先假设要取块中第五个指令。在总线取出第五个字的时候，我们将其输出，由于是猝发，总线将每一个周期送出一条指令，那么我们流水ICache自然也可以持续的将第六个第七个第八个指令送出。在这种处理之下，Interface的总线块寄存器也可以被当作是一个缓存，在流水畅通的情况下，我们可以让时间都花在访存上，预计可以节省10-20%的指令访存时间。但是我们还注意到，我们还会加入指令预取技术，这样的技术实际上会降低缺失率，这使得关键字技术的收益降低，甚至是无效。但是目前我想到了一种合二为一的结构，我们将在**指令预取**部分进行具体介绍。

DCache：由于Interface的从总线读数据步骤统一，对于DCache和ICache都没有差别，因此关键字技术应该可以以相同的方式惠及两者。需要注意的是，DCache不流水也可以使用这样的方式获得比较高的收益。由于访存指令并非同取指一般频繁，因此一般不会出现访存指令连续的情况，所以可以取得比较好的收益，但是实际上我们不能期待这样的性能能够做到非常大的提升，事实上可以预测这只是九牛一毛。整体应该能够提升1-2分性能分。

具体的技术构造将会在流水实现之后给出。

#### ICache

在这一代的ICache中，在实现了双发射的基本适配的情况下，我们再次准备向前迈进一大步。同时我们还要解决相应阻塞问题，让ICache能够连续进行流动，同时我们还要做到指令预取以及双发射功能中的FIFO适配，用以解决边界单发射问题以及在流水过程中pc和单双发指令不匹配问题。

##### ICache流水切分

ICache的流水沿袭了之前的状态机设计，共三级流水线，但是实际上只有两级参与流水，这样我们就能够在命中的时候连续的从cpu的取指阶段接受读指令信号，同时在译码阶段不停的送回指令数据。如果之后深流水线可以变为三级取值，那么之后需要再次进行流水切分设计，将当前并行变为串行。

- LOOK_UP（流水）：将数据赋值于ram取数据，同时这个阶段要处理不命中返回ram的写入操作（该操作会造成数据相关问题）。
- SCAN_CACHE（流水）：对读出的数据进行处理，判断是否命中，命中则输出，不命中则送入不命中额外处理。
- HIT_FAIL（不命中额外处理）：访存，在访存结束的时候输出数据，并且将读出块写回ram。

##### ICache流水控制器

流水控制器是用来解决不命中情况下的流水线情况，合理的计划流水线的流动。对于ICache，我们简单的将其考虑为两个状态，这两个状态为组合逻辑转换。

- WORKING：流水线运行正常，没有不命中发生，检测到不命中进入LOADMEM。这个时候不发出流水线暂停信号，并且HIT_FAIL部分不进行操作，指令输出为SCAN_CACHE级的数据。
- LOADMEM：当前正在处理流水不命中情况，，检测到结束之后进入WORKING。这个时候发出流水线暂停信号，并且将ICache流水线进行暂停。指令输出为总线方给予的数据。

##### ICache结构

对于Cache Ram，我们将其设置为读写双端口，并且对于命名和结构进行调整，先将其适应于非流水状态机ICache，需要注意，采用simple dual port ram，读写同地址进行访问的时候，写优先，下一拍读出的数据为错误数据。

#### 指令预取

指令预取的基本思路是简单的预测到下面我们会对哪些地址进行访存，我们可以趁命中的时候让访存工作起来，这样的话我们就可以节省访存的时间，这种技术通常可以使得命中率再上一个数量级。本质来说，指令预取的效果是一种trade off，并不一定会使得性能得到增强。

但是我们初步的设想是只将其应用至ICache，因为ICache的访存非常直接，程序指令非常大可能是一个循环或者是顺序执行下去，对于前者已经命中，对于后者我们可以很方便的预测到当前指令位于的块的下一个块很有可能使用，因此我们将其预取出来。

### 剩余待实现的功能

- ~~Valid标志的判断~~
- ~~伪LRU~~~~（功能上实现之后进一步优化仿真代码、综合）~~
- ~~DCache~~
- ~~写回型store的实现~~
- ~~写入缓冲FIFO~~
- ~~总线块打包（完整的cache）~~
- Cache指令（pmon要求，满足任老板的一切想象）
- 硬件初始化（单发射Cache的基本要求）
- 流水切割（初具成果的cache）
- 指令预取
- ~~双发射（最终成品的cache的基础版）~~
- 4路组相联16KBcache
- 这之后就是各种提升性能的关键所在了

### 待实现的拓展优化：

#### 双发射同时执行两条访存指令

#### TagV ram采用组合逻辑寄存器

#### VIPT

为了提升cache的查找性能，不被MMU的查找所拖累，首先先使用虚拟地址index进行查找，两者同时完成。之后来比较对应的tag（tag是物理地址，而非虚拟）是否命中。具体见知乎：https://zhuanlan.zhihu.com/p/107096130。

![img](https://pic4.zhimg.com/80/v2-12969d6792ebc4b5256e1822ffde6caf_720w.jpg)

